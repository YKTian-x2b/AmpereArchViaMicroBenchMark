//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32688072
// Cuda compilation tools, release 12.1, V12.1.105
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_86
.address_size 64

	// .globl	_Z15flush_l2_kernelILi128EEvPKiPi
// _ZZ26gloMem_latency_test_kernelPjS_S_E8s_tvalue has been demoted

.visible .entry _Z15flush_l2_kernelILi128EEvPKiPi(
	.param .u64 _Z15flush_l2_kernelILi128EEvPKiPi_param_0,
	.param .u64 _Z15flush_l2_kernelILi128EEvPKiPi_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<102>;
	.reg .b64 	%rd<135>;


	ld.param.u64 	%rd34, [_Z15flush_l2_kernelILi128EEvPKiPi_param_0];
	ld.param.u64 	%rd1, [_Z15flush_l2_kernelILi128EEvPKiPi_param_1];
	mov.u32 	%r66, %tid.x;
	and.b32  	%r67, %r66, -32;
	and.b32  	%r68, %r66, 31;
	mov.u32 	%r69, %ctaid.x;
	shl.b32 	%r70, %r69, 7;
	cvt.u64.u32 	%rd35, %r70;
	cvt.s64.s32 	%rd36, %r67;
	add.s64 	%rd37, %rd35, %rd36;
	cvt.u64.u32 	%rd38, %r68;
	or.b64  	%rd39, %rd37, %rd38;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd2, %rd34, %rd40;
	mov.u32 	%r4, 0;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd2];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r71, %r68, 1;
	cvt.u64.u32 	%rd41, %r71;
	or.b64  	%rd42, %rd37, %rd41;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd3, %rd34, %rd43;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd3];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r72, %r68, 2;
	cvt.u64.u32 	%rd44, %r72;
	or.b64  	%rd45, %rd37, %rd44;
	shl.b64 	%rd46, %rd45, 2;
	add.s64 	%rd4, %rd34, %rd46;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd4];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r73, %r68, 3;
	cvt.u64.u32 	%rd47, %r73;
	or.b64  	%rd48, %rd37, %rd47;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd5, %rd34, %rd49;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd5];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r74, %r68, 4;
	cvt.u64.u32 	%rd50, %r74;
	or.b64  	%rd51, %rd37, %rd50;
	shl.b64 	%rd52, %rd51, 2;
	add.s64 	%rd6, %rd34, %rd52;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd6];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r75, %r68, 5;
	cvt.u64.u32 	%rd53, %r75;
	or.b64  	%rd54, %rd37, %rd53;
	shl.b64 	%rd55, %rd54, 2;
	add.s64 	%rd7, %rd34, %rd55;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd7];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r76, %r68, 6;
	cvt.u64.u32 	%rd56, %r76;
	or.b64  	%rd57, %rd37, %rd56;
	shl.b64 	%rd58, %rd57, 2;
	add.s64 	%rd8, %rd34, %rd58;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd8];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r77, %r68, 7;
	cvt.u64.u32 	%rd59, %r77;
	or.b64  	%rd60, %rd37, %rd59;
	shl.b64 	%rd61, %rd60, 2;
	add.s64 	%rd9, %rd34, %rd61;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd9];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r78, %r68, 8;
	cvt.u64.u32 	%rd62, %r78;
	or.b64  	%rd63, %rd37, %rd62;
	shl.b64 	%rd64, %rd63, 2;
	add.s64 	%rd10, %rd34, %rd64;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd10];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r79, %r68, 9;
	cvt.u64.u32 	%rd65, %r79;
	or.b64  	%rd66, %rd37, %rd65;
	shl.b64 	%rd67, %rd66, 2;
	add.s64 	%rd11, %rd34, %rd67;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd11];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r80, %r68, 10;
	cvt.u64.u32 	%rd68, %r80;
	or.b64  	%rd69, %rd37, %rd68;
	shl.b64 	%rd70, %rd69, 2;
	add.s64 	%rd12, %rd34, %rd70;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd12];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r81, %r68, 11;
	cvt.u64.u32 	%rd71, %r81;
	or.b64  	%rd72, %rd37, %rd71;
	shl.b64 	%rd73, %rd72, 2;
	add.s64 	%rd13, %rd34, %rd73;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd13];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r82, %r68, 12;
	cvt.u64.u32 	%rd74, %r82;
	or.b64  	%rd75, %rd37, %rd74;
	shl.b64 	%rd76, %rd75, 2;
	add.s64 	%rd14, %rd34, %rd76;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd14];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r83, %r68, 13;
	cvt.u64.u32 	%rd77, %r83;
	or.b64  	%rd78, %rd37, %rd77;
	shl.b64 	%rd79, %rd78, 2;
	add.s64 	%rd15, %rd34, %rd79;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd15];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r84, %r68, 14;
	cvt.u64.u32 	%rd80, %r84;
	or.b64  	%rd81, %rd37, %rd80;
	shl.b64 	%rd82, %rd81, 2;
	add.s64 	%rd16, %rd34, %rd82;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd16];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r85, %r68, 15;
	cvt.u64.u32 	%rd83, %r85;
	or.b64  	%rd84, %rd37, %rd83;
	shl.b64 	%rd85, %rd84, 2;
	add.s64 	%rd17, %rd34, %rd85;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd17];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r86, %r68, 16;
	cvt.u64.u32 	%rd86, %r86;
	or.b64  	%rd87, %rd37, %rd86;
	shl.b64 	%rd88, %rd87, 2;
	add.s64 	%rd18, %rd34, %rd88;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd18];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r87, %r68, 17;
	cvt.u64.u32 	%rd89, %r87;
	or.b64  	%rd90, %rd37, %rd89;
	shl.b64 	%rd91, %rd90, 2;
	add.s64 	%rd19, %rd34, %rd91;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd19];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r88, %r68, 18;
	cvt.u64.u32 	%rd92, %r88;
	or.b64  	%rd93, %rd37, %rd92;
	shl.b64 	%rd94, %rd93, 2;
	add.s64 	%rd20, %rd34, %rd94;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd20];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r89, %r68, 19;
	cvt.u64.u32 	%rd95, %r89;
	or.b64  	%rd96, %rd37, %rd95;
	shl.b64 	%rd97, %rd96, 2;
	add.s64 	%rd21, %rd34, %rd97;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd21];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r90, %r68, 20;
	cvt.u64.u32 	%rd98, %r90;
	or.b64  	%rd99, %rd37, %rd98;
	shl.b64 	%rd100, %rd99, 2;
	add.s64 	%rd22, %rd34, %rd100;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd22];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r91, %r68, 21;
	cvt.u64.u32 	%rd101, %r91;
	or.b64  	%rd102, %rd37, %rd101;
	shl.b64 	%rd103, %rd102, 2;
	add.s64 	%rd23, %rd34, %rd103;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd23];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r92, %r68, 22;
	cvt.u64.u32 	%rd104, %r92;
	or.b64  	%rd105, %rd37, %rd104;
	shl.b64 	%rd106, %rd105, 2;
	add.s64 	%rd24, %rd34, %rd106;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd24];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r93, %r68, 23;
	cvt.u64.u32 	%rd107, %r93;
	or.b64  	%rd108, %rd37, %rd107;
	shl.b64 	%rd109, %rd108, 2;
	add.s64 	%rd25, %rd34, %rd109;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd25];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r94, %r68, 24;
	cvt.u64.u32 	%rd110, %r94;
	or.b64  	%rd111, %rd37, %rd110;
	shl.b64 	%rd112, %rd111, 2;
	add.s64 	%rd26, %rd34, %rd112;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd26];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r95, %r68, 25;
	cvt.u64.u32 	%rd113, %r95;
	or.b64  	%rd114, %rd37, %rd113;
	shl.b64 	%rd115, %rd114, 2;
	add.s64 	%rd27, %rd34, %rd115;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd27];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r96, %r68, 26;
	cvt.u64.u32 	%rd116, %r96;
	or.b64  	%rd117, %rd37, %rd116;
	shl.b64 	%rd118, %rd117, 2;
	add.s64 	%rd28, %rd34, %rd118;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd28];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r97, %r68, 27;
	cvt.u64.u32 	%rd119, %r97;
	or.b64  	%rd120, %rd37, %rd119;
	shl.b64 	%rd121, %rd120, 2;
	add.s64 	%rd29, %rd34, %rd121;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd29];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r98, %r68, 28;
	cvt.u64.u32 	%rd122, %r98;
	or.b64  	%rd123, %rd37, %rd122;
	shl.b64 	%rd124, %rd123, 2;
	add.s64 	%rd30, %rd34, %rd124;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd30];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r99, %r68, 29;
	cvt.u64.u32 	%rd125, %r99;
	or.b64  	%rd126, %rd37, %rd125;
	shl.b64 	%rd127, %rd126, 2;
	add.s64 	%rd31, %rd34, %rd127;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd31];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r100, %r68, 30;
	cvt.u64.u32 	%rd128, %r100;
	or.b64  	%rd129, %rd37, %rd128;
	shl.b64 	%rd130, %rd129, 2;
	add.s64 	%rd32, %rd34, %rd130;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd32];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r101, %r68, 31;
	cvt.u64.u32 	%rd131, %r101;
	or.b64  	%rd132, %rd37, %rd131;
	shl.b64 	%rd133, %rd132, 2;
	add.s64 	%rd33, %rd34, %rd133;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd33];
 add.s32 %r4, val, %r4;}

	// end inline asm
	setp.eq.s32 	%p1, %r4, 0;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd134, %rd1;
	st.global.u32 	[%rd134], %r4;

$L__BB0_2:
	ret;

}
	// .globl	_Z26gloMem_latency_test_kernelPjS_S_
.visible .entry _Z26gloMem_latency_test_kernelPjS_S_(
	.param .u64 _Z26gloMem_latency_test_kernelPjS_S__param_0,
	.param .u64 _Z26gloMem_latency_test_kernelPjS_S__param_1,
	.param .u64 _Z26gloMem_latency_test_kernelPjS_S__param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<134>;
	.reg .b64 	%rd<41>;
	// demoted variable
	.shared .align 4 .b8 _ZZ26gloMem_latency_test_kernelPjS_S_E8s_tvalue[1024];

	ld.param.u64 	%rd7, [_Z26gloMem_latency_test_kernelPjS_S__param_0];
	ld.param.u64 	%rd5, [_Z26gloMem_latency_test_kernelPjS_S__param_1];
	ld.param.u64 	%rd6, [_Z26gloMem_latency_test_kernelPjS_S__param_2];
	cvta.to.global.u64 	%rd40, %rd7;
	mov.u32 	%r130, 0;
	mov.u32 	%r129, 32;
	mov.u32 	%r132, _ZZ26gloMem_latency_test_kernelPjS_S_E8s_tvalue;
	mov.u32 	%r131, %r130;

$L__BB1_1:
	// begin inline asm
	mov.u32 %r14, %clock;
	// end inline asm
	mul.wide.u32 	%rd24, %r130, 4;
	add.s64 	%rd8, %rd6, %rd24;
	// begin inline asm
	ld.global.ca.b32 %r15, [%rd8];
	// end inline asm
	add.s32 	%r62, %r15, %r131;
	// begin inline asm
	mov.u32 %r16, %clock;
	// end inline asm
	sub.s32 	%r63, %r16, %r14;
	add.s32 	%r65, %r132, %r129;
	st.shared.u32 	[%r65+-32], %r63;
	// begin inline asm
	mov.u32 %r17, %clock;
	// end inline asm
	mul.wide.u32 	%rd25, %r15, 4;
	add.s64 	%rd9, %rd6, %rd25;
	// begin inline asm
	ld.global.ca.b32 %r18, [%rd9];
	// end inline asm
	add.s32 	%r66, %r18, %r62;
	// begin inline asm
	mov.u32 %r19, %clock;
	// end inline asm
	sub.s32 	%r67, %r19, %r17;
	st.shared.u32 	[%r65+-28], %r67;
	// begin inline asm
	mov.u32 %r20, %clock;
	// end inline asm
	mul.wide.u32 	%rd26, %r18, 4;
	add.s64 	%rd10, %rd6, %rd26;
	// begin inline asm
	ld.global.ca.b32 %r21, [%rd10];
	// end inline asm
	add.s32 	%r68, %r21, %r66;
	// begin inline asm
	mov.u32 %r22, %clock;
	// end inline asm
	sub.s32 	%r69, %r22, %r20;
	st.shared.u32 	[%r65+-24], %r69;
	// begin inline asm
	mov.u32 %r23, %clock;
	// end inline asm
	mul.wide.u32 	%rd27, %r21, 4;
	add.s64 	%rd11, %rd6, %rd27;
	// begin inline asm
	ld.global.ca.b32 %r24, [%rd11];
	// end inline asm
	add.s32 	%r70, %r24, %r68;
	// begin inline asm
	mov.u32 %r25, %clock;
	// end inline asm
	sub.s32 	%r71, %r25, %r23;
	st.shared.u32 	[%r65+-20], %r71;
	// begin inline asm
	mov.u32 %r26, %clock;
	// end inline asm
	mul.wide.u32 	%rd28, %r24, 4;
	add.s64 	%rd12, %rd6, %rd28;
	// begin inline asm
	ld.global.ca.b32 %r27, [%rd12];
	// end inline asm
	add.s32 	%r72, %r27, %r70;
	// begin inline asm
	mov.u32 %r28, %clock;
	// end inline asm
	sub.s32 	%r73, %r28, %r26;
	st.shared.u32 	[%r65+-16], %r73;
	// begin inline asm
	mov.u32 %r29, %clock;
	// end inline asm
	mul.wide.u32 	%rd29, %r27, 4;
	add.s64 	%rd13, %rd6, %rd29;
	// begin inline asm
	ld.global.ca.b32 %r30, [%rd13];
	// end inline asm
	add.s32 	%r74, %r30, %r72;
	// begin inline asm
	mov.u32 %r31, %clock;
	// end inline asm
	sub.s32 	%r75, %r31, %r29;
	st.shared.u32 	[%r65+-12], %r75;
	// begin inline asm
	mov.u32 %r32, %clock;
	// end inline asm
	mul.wide.u32 	%rd30, %r30, 4;
	add.s64 	%rd14, %rd6, %rd30;
	// begin inline asm
	ld.global.ca.b32 %r33, [%rd14];
	// end inline asm
	add.s32 	%r76, %r33, %r74;
	// begin inline asm
	mov.u32 %r34, %clock;
	// end inline asm
	sub.s32 	%r77, %r34, %r32;
	st.shared.u32 	[%r65+-8], %r77;
	// begin inline asm
	mov.u32 %r35, %clock;
	// end inline asm
	mul.wide.u32 	%rd31, %r33, 4;
	add.s64 	%rd15, %rd6, %rd31;
	// begin inline asm
	ld.global.ca.b32 %r36, [%rd15];
	// end inline asm
	add.s32 	%r78, %r36, %r76;
	// begin inline asm
	mov.u32 %r37, %clock;
	// end inline asm
	sub.s32 	%r79, %r37, %r35;
	st.shared.u32 	[%r65+-4], %r79;
	// begin inline asm
	mov.u32 %r38, %clock;
	// end inline asm
	mul.wide.u32 	%rd32, %r36, 4;
	add.s64 	%rd16, %rd6, %rd32;
	// begin inline asm
	ld.global.ca.b32 %r39, [%rd16];
	// end inline asm
	add.s32 	%r80, %r39, %r78;
	// begin inline asm
	mov.u32 %r40, %clock;
	// end inline asm
	sub.s32 	%r81, %r40, %r38;
	st.shared.u32 	[%r65], %r81;
	// begin inline asm
	mov.u32 %r41, %clock;
	// end inline asm
	mul.wide.u32 	%rd33, %r39, 4;
	add.s64 	%rd17, %rd6, %rd33;
	// begin inline asm
	ld.global.ca.b32 %r42, [%rd17];
	// end inline asm
	add.s32 	%r82, %r42, %r80;
	// begin inline asm
	mov.u32 %r43, %clock;
	// end inline asm
	sub.s32 	%r83, %r43, %r41;
	st.shared.u32 	[%r65+4], %r83;
	// begin inline asm
	mov.u32 %r44, %clock;
	// end inline asm
	mul.wide.u32 	%rd34, %r42, 4;
	add.s64 	%rd18, %rd6, %rd34;
	// begin inline asm
	ld.global.ca.b32 %r45, [%rd18];
	// end inline asm
	add.s32 	%r84, %r45, %r82;
	// begin inline asm
	mov.u32 %r46, %clock;
	// end inline asm
	sub.s32 	%r85, %r46, %r44;
	st.shared.u32 	[%r65+8], %r85;
	// begin inline asm
	mov.u32 %r47, %clock;
	// end inline asm
	mul.wide.u32 	%rd35, %r45, 4;
	add.s64 	%rd19, %rd6, %rd35;
	// begin inline asm
	ld.global.ca.b32 %r48, [%rd19];
	// end inline asm
	add.s32 	%r86, %r48, %r84;
	// begin inline asm
	mov.u32 %r49, %clock;
	// end inline asm
	sub.s32 	%r87, %r49, %r47;
	st.shared.u32 	[%r65+12], %r87;
	// begin inline asm
	mov.u32 %r50, %clock;
	// end inline asm
	mul.wide.u32 	%rd36, %r48, 4;
	add.s64 	%rd20, %rd6, %rd36;
	// begin inline asm
	ld.global.ca.b32 %r51, [%rd20];
	// end inline asm
	add.s32 	%r88, %r51, %r86;
	// begin inline asm
	mov.u32 %r52, %clock;
	// end inline asm
	sub.s32 	%r89, %r52, %r50;
	st.shared.u32 	[%r65+16], %r89;
	// begin inline asm
	mov.u32 %r53, %clock;
	// end inline asm
	mul.wide.u32 	%rd37, %r51, 4;
	add.s64 	%rd21, %rd6, %rd37;
	// begin inline asm
	ld.global.ca.b32 %r54, [%rd21];
	// end inline asm
	add.s32 	%r90, %r54, %r88;
	// begin inline asm
	mov.u32 %r55, %clock;
	// end inline asm
	sub.s32 	%r91, %r55, %r53;
	st.shared.u32 	[%r65+20], %r91;
	// begin inline asm
	mov.u32 %r56, %clock;
	// end inline asm
	mul.wide.u32 	%rd38, %r54, 4;
	add.s64 	%rd22, %rd6, %rd38;
	// begin inline asm
	ld.global.ca.b32 %r57, [%rd22];
	// end inline asm
	add.s32 	%r92, %r57, %r90;
	// begin inline asm
	mov.u32 %r58, %clock;
	// end inline asm
	sub.s32 	%r93, %r58, %r56;
	st.shared.u32 	[%r65+24], %r93;
	// begin inline asm
	mov.u32 %r59, %clock;
	// end inline asm
	mul.wide.u32 	%rd39, %r57, 4;
	add.s64 	%rd23, %rd6, %rd39;
	// begin inline asm
	ld.global.ca.b32 %r130, [%rd23];
	// end inline asm
	add.s32 	%r131, %r130, %r92;
	// begin inline asm
	mov.u32 %r61, %clock;
	// end inline asm
	sub.s32 	%r94, %r61, %r59;
	st.shared.u32 	[%r65+28], %r94;
	add.s32 	%r129, %r129, 64;
	setp.ne.s32 	%p1, %r129, 1056;
	@%p1 bra 	$L__BB1_1;

	cvta.to.global.u64 	%rd2, %rd5;
	mov.u32 	%r133, 0;

$L__BB1_3:
	ld.shared.u32 	%r97, [%r132];
	st.global.u32 	[%rd40], %r97;
	ld.shared.u32 	%r98, [%r132+4];
	st.global.u32 	[%rd40+4], %r98;
	ld.shared.u32 	%r99, [%r132+8];
	st.global.u32 	[%rd40+8], %r99;
	ld.shared.u32 	%r100, [%r132+12];
	st.global.u32 	[%rd40+12], %r100;
	ld.shared.u32 	%r101, [%r132+16];
	st.global.u32 	[%rd40+16], %r101;
	ld.shared.u32 	%r102, [%r132+20];
	st.global.u32 	[%rd40+20], %r102;
	ld.shared.u32 	%r103, [%r132+24];
	st.global.u32 	[%rd40+24], %r103;
	ld.shared.u32 	%r104, [%r132+28];
	st.global.u32 	[%rd40+28], %r104;
	ld.shared.u32 	%r105, [%r132+32];
	st.global.u32 	[%rd40+32], %r105;
	ld.shared.u32 	%r106, [%r132+36];
	st.global.u32 	[%rd40+36], %r106;
	ld.shared.u32 	%r107, [%r132+40];
	st.global.u32 	[%rd40+40], %r107;
	ld.shared.u32 	%r108, [%r132+44];
	st.global.u32 	[%rd40+44], %r108;
	ld.shared.u32 	%r109, [%r132+48];
	st.global.u32 	[%rd40+48], %r109;
	ld.shared.u32 	%r110, [%r132+52];
	st.global.u32 	[%rd40+52], %r110;
	ld.shared.u32 	%r111, [%r132+56];
	st.global.u32 	[%rd40+56], %r111;
	ld.shared.u32 	%r112, [%r132+60];
	st.global.u32 	[%rd40+60], %r112;
	ld.shared.u32 	%r113, [%r132+64];
	st.global.u32 	[%rd40+64], %r113;
	ld.shared.u32 	%r114, [%r132+68];
	st.global.u32 	[%rd40+68], %r114;
	ld.shared.u32 	%r115, [%r132+72];
	st.global.u32 	[%rd40+72], %r115;
	ld.shared.u32 	%r116, [%r132+76];
	st.global.u32 	[%rd40+76], %r116;
	ld.shared.u32 	%r117, [%r132+80];
	st.global.u32 	[%rd40+80], %r117;
	ld.shared.u32 	%r118, [%r132+84];
	st.global.u32 	[%rd40+84], %r118;
	ld.shared.u32 	%r119, [%r132+88];
	st.global.u32 	[%rd40+88], %r119;
	ld.shared.u32 	%r120, [%r132+92];
	st.global.u32 	[%rd40+92], %r120;
	ld.shared.u32 	%r121, [%r132+96];
	st.global.u32 	[%rd40+96], %r121;
	ld.shared.u32 	%r122, [%r132+100];
	st.global.u32 	[%rd40+100], %r122;
	ld.shared.u32 	%r123, [%r132+104];
	st.global.u32 	[%rd40+104], %r123;
	ld.shared.u32 	%r124, [%r132+108];
	st.global.u32 	[%rd40+108], %r124;
	ld.shared.u32 	%r125, [%r132+112];
	st.global.u32 	[%rd40+112], %r125;
	ld.shared.u32 	%r126, [%r132+116];
	st.global.u32 	[%rd40+116], %r126;
	ld.shared.u32 	%r127, [%r132+120];
	st.global.u32 	[%rd40+120], %r127;
	ld.shared.u32 	%r128, [%r132+124];
	st.global.u32 	[%rd40+124], %r128;
	add.s32 	%r132, %r132, 128;
	add.s64 	%rd40, %rd40, 128;
	add.s32 	%r133, %r133, 32;
	setp.ne.s32 	%p2, %r133, 256;
	@%p2 bra 	$L__BB1_3;

	st.global.u32 	[%rd2], %r131;
	ret;

}


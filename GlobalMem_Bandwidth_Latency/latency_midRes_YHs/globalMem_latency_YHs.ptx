//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32688072
// Cuda compilation tools, release 12.1, V12.1.105
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_86
.address_size 64

	// .globl	_Z15flush_l2_kernelILi128EEvPKiPi

.visible .entry _Z15flush_l2_kernelILi128EEvPKiPi(
	.param .u64 _Z15flush_l2_kernelILi128EEvPKiPi_param_0,
	.param .u64 _Z15flush_l2_kernelILi128EEvPKiPi_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<102>;
	.reg .b64 	%rd<135>;


	ld.param.u64 	%rd34, [_Z15flush_l2_kernelILi128EEvPKiPi_param_0];
	ld.param.u64 	%rd1, [_Z15flush_l2_kernelILi128EEvPKiPi_param_1];
	mov.u32 	%r66, %tid.x;
	and.b32  	%r67, %r66, -32;
	and.b32  	%r68, %r66, 31;
	mov.u32 	%r69, %ctaid.x;
	shl.b32 	%r70, %r69, 7;
	cvt.u64.u32 	%rd35, %r70;
	cvt.s64.s32 	%rd36, %r67;
	add.s64 	%rd37, %rd35, %rd36;
	cvt.u64.u32 	%rd38, %r68;
	or.b64  	%rd39, %rd37, %rd38;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd2, %rd34, %rd40;
	mov.u32 	%r4, 0;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd2];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r71, %r68, 1;
	cvt.u64.u32 	%rd41, %r71;
	or.b64  	%rd42, %rd37, %rd41;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd3, %rd34, %rd43;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd3];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r72, %r68, 2;
	cvt.u64.u32 	%rd44, %r72;
	or.b64  	%rd45, %rd37, %rd44;
	shl.b64 	%rd46, %rd45, 2;
	add.s64 	%rd4, %rd34, %rd46;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd4];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r73, %r68, 3;
	cvt.u64.u32 	%rd47, %r73;
	or.b64  	%rd48, %rd37, %rd47;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd5, %rd34, %rd49;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd5];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r74, %r68, 4;
	cvt.u64.u32 	%rd50, %r74;
	or.b64  	%rd51, %rd37, %rd50;
	shl.b64 	%rd52, %rd51, 2;
	add.s64 	%rd6, %rd34, %rd52;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd6];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r75, %r68, 5;
	cvt.u64.u32 	%rd53, %r75;
	or.b64  	%rd54, %rd37, %rd53;
	shl.b64 	%rd55, %rd54, 2;
	add.s64 	%rd7, %rd34, %rd55;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd7];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r76, %r68, 6;
	cvt.u64.u32 	%rd56, %r76;
	or.b64  	%rd57, %rd37, %rd56;
	shl.b64 	%rd58, %rd57, 2;
	add.s64 	%rd8, %rd34, %rd58;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd8];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r77, %r68, 7;
	cvt.u64.u32 	%rd59, %r77;
	or.b64  	%rd60, %rd37, %rd59;
	shl.b64 	%rd61, %rd60, 2;
	add.s64 	%rd9, %rd34, %rd61;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd9];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r78, %r68, 8;
	cvt.u64.u32 	%rd62, %r78;
	or.b64  	%rd63, %rd37, %rd62;
	shl.b64 	%rd64, %rd63, 2;
	add.s64 	%rd10, %rd34, %rd64;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd10];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r79, %r68, 9;
	cvt.u64.u32 	%rd65, %r79;
	or.b64  	%rd66, %rd37, %rd65;
	shl.b64 	%rd67, %rd66, 2;
	add.s64 	%rd11, %rd34, %rd67;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd11];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r80, %r68, 10;
	cvt.u64.u32 	%rd68, %r80;
	or.b64  	%rd69, %rd37, %rd68;
	shl.b64 	%rd70, %rd69, 2;
	add.s64 	%rd12, %rd34, %rd70;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd12];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r81, %r68, 11;
	cvt.u64.u32 	%rd71, %r81;
	or.b64  	%rd72, %rd37, %rd71;
	shl.b64 	%rd73, %rd72, 2;
	add.s64 	%rd13, %rd34, %rd73;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd13];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r82, %r68, 12;
	cvt.u64.u32 	%rd74, %r82;
	or.b64  	%rd75, %rd37, %rd74;
	shl.b64 	%rd76, %rd75, 2;
	add.s64 	%rd14, %rd34, %rd76;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd14];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r83, %r68, 13;
	cvt.u64.u32 	%rd77, %r83;
	or.b64  	%rd78, %rd37, %rd77;
	shl.b64 	%rd79, %rd78, 2;
	add.s64 	%rd15, %rd34, %rd79;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd15];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r84, %r68, 14;
	cvt.u64.u32 	%rd80, %r84;
	or.b64  	%rd81, %rd37, %rd80;
	shl.b64 	%rd82, %rd81, 2;
	add.s64 	%rd16, %rd34, %rd82;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd16];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r85, %r68, 15;
	cvt.u64.u32 	%rd83, %r85;
	or.b64  	%rd84, %rd37, %rd83;
	shl.b64 	%rd85, %rd84, 2;
	add.s64 	%rd17, %rd34, %rd85;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd17];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r86, %r68, 16;
	cvt.u64.u32 	%rd86, %r86;
	or.b64  	%rd87, %rd37, %rd86;
	shl.b64 	%rd88, %rd87, 2;
	add.s64 	%rd18, %rd34, %rd88;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd18];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r87, %r68, 17;
	cvt.u64.u32 	%rd89, %r87;
	or.b64  	%rd90, %rd37, %rd89;
	shl.b64 	%rd91, %rd90, 2;
	add.s64 	%rd19, %rd34, %rd91;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd19];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r88, %r68, 18;
	cvt.u64.u32 	%rd92, %r88;
	or.b64  	%rd93, %rd37, %rd92;
	shl.b64 	%rd94, %rd93, 2;
	add.s64 	%rd20, %rd34, %rd94;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd20];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r89, %r68, 19;
	cvt.u64.u32 	%rd95, %r89;
	or.b64  	%rd96, %rd37, %rd95;
	shl.b64 	%rd97, %rd96, 2;
	add.s64 	%rd21, %rd34, %rd97;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd21];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r90, %r68, 20;
	cvt.u64.u32 	%rd98, %r90;
	or.b64  	%rd99, %rd37, %rd98;
	shl.b64 	%rd100, %rd99, 2;
	add.s64 	%rd22, %rd34, %rd100;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd22];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r91, %r68, 21;
	cvt.u64.u32 	%rd101, %r91;
	or.b64  	%rd102, %rd37, %rd101;
	shl.b64 	%rd103, %rd102, 2;
	add.s64 	%rd23, %rd34, %rd103;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd23];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r92, %r68, 22;
	cvt.u64.u32 	%rd104, %r92;
	or.b64  	%rd105, %rd37, %rd104;
	shl.b64 	%rd106, %rd105, 2;
	add.s64 	%rd24, %rd34, %rd106;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd24];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r93, %r68, 23;
	cvt.u64.u32 	%rd107, %r93;
	or.b64  	%rd108, %rd37, %rd107;
	shl.b64 	%rd109, %rd108, 2;
	add.s64 	%rd25, %rd34, %rd109;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd25];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r94, %r68, 24;
	cvt.u64.u32 	%rd110, %r94;
	or.b64  	%rd111, %rd37, %rd110;
	shl.b64 	%rd112, %rd111, 2;
	add.s64 	%rd26, %rd34, %rd112;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd26];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r95, %r68, 25;
	cvt.u64.u32 	%rd113, %r95;
	or.b64  	%rd114, %rd37, %rd113;
	shl.b64 	%rd115, %rd114, 2;
	add.s64 	%rd27, %rd34, %rd115;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd27];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r96, %r68, 26;
	cvt.u64.u32 	%rd116, %r96;
	or.b64  	%rd117, %rd37, %rd116;
	shl.b64 	%rd118, %rd117, 2;
	add.s64 	%rd28, %rd34, %rd118;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd28];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r97, %r68, 27;
	cvt.u64.u32 	%rd119, %r97;
	or.b64  	%rd120, %rd37, %rd119;
	shl.b64 	%rd121, %rd120, 2;
	add.s64 	%rd29, %rd34, %rd121;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd29];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r98, %r68, 28;
	cvt.u64.u32 	%rd122, %r98;
	or.b64  	%rd123, %rd37, %rd122;
	shl.b64 	%rd124, %rd123, 2;
	add.s64 	%rd30, %rd34, %rd124;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd30];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r99, %r68, 29;
	cvt.u64.u32 	%rd125, %r99;
	or.b64  	%rd126, %rd37, %rd125;
	shl.b64 	%rd127, %rd126, 2;
	add.s64 	%rd31, %rd34, %rd127;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd31];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r100, %r68, 30;
	cvt.u64.u32 	%rd128, %r100;
	or.b64  	%rd129, %rd37, %rd128;
	shl.b64 	%rd130, %rd129, 2;
	add.s64 	%rd32, %rd34, %rd130;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd32];
 add.s32 %r4, val, %r4;}

	// end inline asm
	xor.b32  	%r101, %r68, 31;
	cvt.u64.u32 	%rd131, %r101;
	or.b64  	%rd132, %rd37, %rd131;
	shl.b64 	%rd133, %rd132, 2;
	add.s64 	%rd33, %rd34, %rd133;
	// begin inline asm
	{.reg .s32 val;
 ld.global.cg.b32 val, [%rd33];
 add.s32 %r4, val, %r4;}

	// end inline asm
	setp.eq.s32 	%p1, %r4, 0;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd134, %rd1;
	st.global.u32 	[%rd134], %r4;

$L__BB0_2:
	ret;

}
	// .globl	_Z19dram_latency_kernelILi10EEvPKjPjS2_
.visible .entry _Z19dram_latency_kernelILi10EEvPKjPjS2_(
	.param .u64 _Z19dram_latency_kernelILi10EEvPKjPjS2__param_0,
	.param .u64 _Z19dram_latency_kernelILi10EEvPKjPjS2__param_1,
	.param .u64 _Z19dram_latency_kernelILi10EEvPKjPjS2__param_2
)
.maxntid 32, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<29>;


	ld.param.u64 	%rd13, [_Z19dram_latency_kernelILi10EEvPKjPjS2__param_0];
	ld.param.u64 	%rd1, [_Z19dram_latency_kernelILi10EEvPKjPjS2__param_1];
	ld.param.u64 	%rd14, [_Z19dram_latency_kernelILi10EEvPKjPjS2__param_2];
	mov.u32 	%r14, %tid.x;
	mul.wide.u32 	%rd15, %r14, 4;
	add.s64 	%rd2, %rd13, %rd15;
	// begin inline asm
	ld.global.cg.b32 %r1, [%rd2];

	// end inline asm
	cvt.u64.u32 	%rd16, %r1;
	add.s64 	%rd3, %rd2, %rd16;
	// begin inline asm
	bar.sync 0;
mov.u32 %r2, %clock;

	// end inline asm
	// begin inline asm
	ld.global.cg.b32 %r3, [%rd3];

	// end inline asm
	cvt.u64.u32 	%rd17, %r3;
	add.s64 	%rd4, %rd3, %rd17;
	// begin inline asm
	ld.global.cg.b32 %r4, [%rd4];

	// end inline asm
	cvt.u64.u32 	%rd18, %r4;
	add.s64 	%rd5, %rd4, %rd18;
	// begin inline asm
	ld.global.cg.b32 %r5, [%rd5];

	// end inline asm
	cvt.u64.u32 	%rd19, %r5;
	add.s64 	%rd6, %rd5, %rd19;
	// begin inline asm
	ld.global.cg.b32 %r6, [%rd6];

	// end inline asm
	cvt.u64.u32 	%rd20, %r6;
	add.s64 	%rd7, %rd6, %rd20;
	// begin inline asm
	ld.global.cg.b32 %r7, [%rd7];

	// end inline asm
	cvt.u64.u32 	%rd21, %r7;
	add.s64 	%rd8, %rd7, %rd21;
	// begin inline asm
	ld.global.cg.b32 %r8, [%rd8];

	// end inline asm
	cvt.u64.u32 	%rd22, %r8;
	add.s64 	%rd9, %rd8, %rd22;
	// begin inline asm
	ld.global.cg.b32 %r9, [%rd9];

	// end inline asm
	cvt.u64.u32 	%rd23, %r9;
	add.s64 	%rd10, %rd9, %rd23;
	// begin inline asm
	ld.global.cg.b32 %r10, [%rd10];

	// end inline asm
	cvt.u64.u32 	%rd24, %r10;
	add.s64 	%rd11, %rd10, %rd24;
	// begin inline asm
	ld.global.cg.b32 %r11, [%rd11];

	// end inline asm
	cvt.u64.u32 	%rd25, %r11;
	add.s64 	%rd12, %rd11, %rd25;
	// begin inline asm
	ld.global.cg.b32 %r12, [%rd12];

	// end inline asm
	// begin inline asm
	bar.sync 0;
mov.u32 %r13, %clock;

	// end inline asm
	sub.s32 	%r15, %r13, %r2;
	cvta.to.global.u64 	%rd26, %rd14;
	add.s64 	%rd27, %rd26, %rd15;
	st.global.u32 	[%rd27], %r15;
	setp.ne.s32 	%p1, %r12, 0;
	@%p1 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd28, %rd1;
	mov.u32 	%r16, 0;
	st.global.u32 	[%rd28], %r16;

$L__BB1_2:
	ret;

}


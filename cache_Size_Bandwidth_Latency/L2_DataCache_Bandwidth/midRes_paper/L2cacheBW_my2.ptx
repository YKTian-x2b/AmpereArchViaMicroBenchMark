//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32688072
// Cuda compilation tools, release 12.1, V12.1.105
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_86
.address_size 64

	// .globl	_Z5l2_bwPfS_

.visible .entry _Z5l2_bwPfS_(
	.param .u64 _Z5l2_bwPfS__param_0,
	.param .u64 _Z5l2_bwPfS__param_1
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<69>;
	.reg .b32 	%r<130>;
	.reg .b64 	%rd<74>;


	ld.param.u64 	%rd1, [_Z5l2_bwPfS__param_0];
	ld.param.u64 	%rd2, [_Z5l2_bwPfS__param_1];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	shl.b32 	%r127, %r2, 9;
	and.b32  	%r4, %r1, 511;
	add.s32 	%r44, %r1, 32;
	and.b32  	%r5, %r44, 511;
	add.s32 	%r45, %r1, 64;
	and.b32  	%r6, %r45, 511;
	add.s32 	%r46, %r1, 96;
	and.b32  	%r7, %r46, 511;
	add.s32 	%r47, %r1, 128;
	and.b32  	%r8, %r47, 511;
	add.s32 	%r48, %r1, 160;
	and.b32  	%r9, %r48, 511;
	add.s32 	%r49, %r1, 192;
	and.b32  	%r10, %r49, 511;
	add.s32 	%r50, %r1, 224;
	and.b32  	%r11, %r50, 511;
	add.s32 	%r51, %r1, 256;
	and.b32  	%r12, %r51, 511;
	add.s32 	%r52, %r1, 288;
	and.b32  	%r13, %r52, 511;
	add.s32 	%r53, %r1, 320;
	and.b32  	%r14, %r53, 511;
	add.s32 	%r54, %r1, 352;
	and.b32  	%r15, %r54, 511;
	add.s32 	%r55, %r1, 384;
	and.b32  	%r16, %r55, 511;
	add.s32 	%r56, %r1, 416;
	and.b32  	%r17, %r56, 511;
	add.s32 	%r57, %r1, 448;
	and.b32  	%r18, %r57, 511;
	add.s32 	%r58, %r1, 480;
	and.b32  	%r19, %r58, 511;
	add.s32 	%r128, %r127, 512;
	or.b32  	%r21, %r19, 512;
	or.b32  	%r22, %r18, 512;
	or.b32  	%r23, %r17, 512;
	or.b32  	%r24, %r16, 512;
	or.b32  	%r25, %r15, 512;
	or.b32  	%r26, %r14, 512;
	or.b32  	%r27, %r13, 512;
	or.b32  	%r28, %r12, 512;
	or.b32  	%r29, %r11, 512;
	or.b32  	%r30, %r10, 512;
	or.b32  	%r31, %r9, 512;
	or.b32  	%r32, %r8, 512;
	or.b32  	%r33, %r7, 512;
	or.b32  	%r34, %r6, 512;
	or.b32  	%r35, %r5, 512;
	or.b32  	%r36, %r4, 512;
	mov.f32 	%f68, 0f00000000;
	mov.u32 	%r129, 0;

$L__BB0_1:
	mul.wide.u32 	%rd35, %r128, -1851608123;
	shr.u64 	%rd36, %rd35, 50;
	cvt.u32.u64 	%r59, %rd36;
	mul.lo.s32 	%r60, %r59, -460800;
	mul.wide.u32 	%rd37, %r127, -1851608123;
	shr.u64 	%rd38, %rd37, 50;
	cvt.u32.u64 	%r61, %rd38;
	mul.lo.s32 	%r62, %r61, -460800;
	add.s32 	%r63, %r4, %r127;
	add.s32 	%r64, %r63, %r62;
	mul.wide.u32 	%rd39, %r64, 4;
	add.s64 	%rd3, %rd2, %rd39;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd3];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r65, %r5, %r127;
	add.s32 	%r66, %r65, %r62;
	mul.wide.u32 	%rd40, %r66, 4;
	add.s64 	%rd4, %rd2, %rd40;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd4];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r67, %r6, %r127;
	add.s32 	%r68, %r67, %r62;
	mul.wide.u32 	%rd41, %r68, 4;
	add.s64 	%rd5, %rd2, %rd41;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd5];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r69, %r7, %r127;
	add.s32 	%r70, %r69, %r62;
	mul.wide.u32 	%rd42, %r70, 4;
	add.s64 	%rd6, %rd2, %rd42;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd6];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r71, %r8, %r127;
	add.s32 	%r72, %r71, %r62;
	mul.wide.u32 	%rd43, %r72, 4;
	add.s64 	%rd7, %rd2, %rd43;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd7];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r73, %r9, %r127;
	add.s32 	%r74, %r73, %r62;
	mul.wide.u32 	%rd44, %r74, 4;
	add.s64 	%rd8, %rd2, %rd44;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd8];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r75, %r10, %r127;
	add.s32 	%r76, %r75, %r62;
	mul.wide.u32 	%rd45, %r76, 4;
	add.s64 	%rd9, %rd2, %rd45;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd9];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r77, %r11, %r127;
	add.s32 	%r78, %r77, %r62;
	mul.wide.u32 	%rd46, %r78, 4;
	add.s64 	%rd10, %rd2, %rd46;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd10];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r79, %r12, %r127;
	add.s32 	%r80, %r79, %r62;
	mul.wide.u32 	%rd47, %r80, 4;
	add.s64 	%rd11, %rd2, %rd47;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd11];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r81, %r13, %r127;
	add.s32 	%r82, %r81, %r62;
	mul.wide.u32 	%rd48, %r82, 4;
	add.s64 	%rd12, %rd2, %rd48;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd12];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r83, %r14, %r127;
	add.s32 	%r84, %r83, %r62;
	mul.wide.u32 	%rd49, %r84, 4;
	add.s64 	%rd13, %rd2, %rd49;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd13];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r85, %r15, %r127;
	add.s32 	%r86, %r85, %r62;
	mul.wide.u32 	%rd50, %r86, 4;
	add.s64 	%rd14, %rd2, %rd50;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd14];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r87, %r16, %r127;
	add.s32 	%r88, %r87, %r62;
	mul.wide.u32 	%rd51, %r88, 4;
	add.s64 	%rd15, %rd2, %rd51;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd15];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r89, %r17, %r127;
	add.s32 	%r90, %r89, %r62;
	mul.wide.u32 	%rd52, %r90, 4;
	add.s64 	%rd16, %rd2, %rd52;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd16];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r91, %r18, %r127;
	add.s32 	%r92, %r91, %r62;
	mul.wide.u32 	%rd53, %r92, 4;
	add.s64 	%rd17, %rd2, %rd53;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd17];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r93, %r19, %r127;
	add.s32 	%r94, %r93, %r62;
	mul.wide.u32 	%rd54, %r94, 4;
	add.s64 	%rd18, %rd2, %rd54;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd18];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r95, %r36, %r127;
	add.s32 	%r96, %r95, %r60;
	mul.wide.u32 	%rd55, %r96, 4;
	add.s64 	%rd19, %rd2, %rd55;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd19];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r97, %r35, %r127;
	add.s32 	%r98, %r97, %r60;
	mul.wide.u32 	%rd56, %r98, 4;
	add.s64 	%rd20, %rd2, %rd56;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd20];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r99, %r34, %r127;
	add.s32 	%r100, %r99, %r60;
	mul.wide.u32 	%rd57, %r100, 4;
	add.s64 	%rd21, %rd2, %rd57;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd21];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r101, %r33, %r127;
	add.s32 	%r102, %r101, %r60;
	mul.wide.u32 	%rd58, %r102, 4;
	add.s64 	%rd22, %rd2, %rd58;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd22];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r103, %r32, %r127;
	add.s32 	%r104, %r103, %r60;
	mul.wide.u32 	%rd59, %r104, 4;
	add.s64 	%rd23, %rd2, %rd59;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd23];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r105, %r31, %r127;
	add.s32 	%r106, %r105, %r60;
	mul.wide.u32 	%rd60, %r106, 4;
	add.s64 	%rd24, %rd2, %rd60;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd24];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r107, %r30, %r127;
	add.s32 	%r108, %r107, %r60;
	mul.wide.u32 	%rd61, %r108, 4;
	add.s64 	%rd25, %rd2, %rd61;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd25];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r109, %r29, %r127;
	add.s32 	%r110, %r109, %r60;
	mul.wide.u32 	%rd62, %r110, 4;
	add.s64 	%rd26, %rd2, %rd62;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd26];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r111, %r28, %r127;
	add.s32 	%r112, %r111, %r60;
	mul.wide.u32 	%rd63, %r112, 4;
	add.s64 	%rd27, %rd2, %rd63;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd27];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r113, %r27, %r127;
	add.s32 	%r114, %r113, %r60;
	mul.wide.u32 	%rd64, %r114, 4;
	add.s64 	%rd28, %rd2, %rd64;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd28];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r115, %r26, %r127;
	add.s32 	%r116, %r115, %r60;
	mul.wide.u32 	%rd65, %r116, 4;
	add.s64 	%rd29, %rd2, %rd65;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd29];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r117, %r25, %r127;
	add.s32 	%r118, %r117, %r60;
	mul.wide.u32 	%rd66, %r118, 4;
	add.s64 	%rd30, %rd2, %rd66;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd30];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r119, %r24, %r127;
	add.s32 	%r120, %r119, %r60;
	mul.wide.u32 	%rd67, %r120, 4;
	add.s64 	%rd31, %rd2, %rd67;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd31];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r121, %r23, %r127;
	add.s32 	%r122, %r121, %r60;
	mul.wide.u32 	%rd68, %r122, 4;
	add.s64 	%rd32, %rd2, %rd68;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd32];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r123, %r22, %r127;
	add.s32 	%r124, %r123, %r60;
	mul.wide.u32 	%rd69, %r124, 4;
	add.s64 	%rd33, %rd2, %rd69;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd33];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r125, %r21, %r127;
	add.s32 	%r126, %r125, %r60;
	mul.wide.u32 	%rd70, %r126, 4;
	add.s64 	%rd34, %rd2, %rd70;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd34];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s32 	%r129, %r129, 1024;
	setp.lt.u32 	%p1, %r129, 460800;
	add.s32 	%r41, %r128, 1024;
	add.s32 	%r127, %r128, 512;
	mov.u32 	%r128, %r41;
	@%p1 bra 	$L__BB0_1;

	setp.ne.s32 	%p2, %r2, 0;
	@%p2 bra 	$L__BB0_4;

	cvta.to.global.u64 	%rd71, %rd1;
	mul.wide.u32 	%rd72, %r1, 4;
	add.s64 	%rd73, %rd71, %rd72;
	st.global.f32 	[%rd73], %f68;

$L__BB0_4:
	ret;

}
	// .globl	_Z6l2_bw2PfS_
.visible .entry _Z6l2_bw2PfS_(
	.param .u64 _Z6l2_bw2PfS__param_0,
	.param .u64 _Z6l2_bw2PfS__param_1
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<36>;
	.reg .b32 	%r<76>;
	.reg .b64 	%rd<40>;


	ld.param.u64 	%rd1, [_Z6l2_bw2PfS__param_0];
	ld.param.u64 	%rd2, [_Z6l2_bw2PfS__param_1];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	shl.b32 	%r74, %r2, 9;
	and.b32  	%r4, %r1, 511;
	add.s32 	%r25, %r1, 32;
	and.b32  	%r5, %r25, 511;
	add.s32 	%r26, %r1, 64;
	and.b32  	%r6, %r26, 511;
	add.s32 	%r27, %r1, 96;
	and.b32  	%r7, %r27, 511;
	add.s32 	%r28, %r1, 128;
	and.b32  	%r8, %r28, 511;
	add.s32 	%r29, %r1, 160;
	and.b32  	%r9, %r29, 511;
	add.s32 	%r30, %r1, 192;
	and.b32  	%r10, %r30, 511;
	add.s32 	%r31, %r1, 224;
	and.b32  	%r11, %r31, 511;
	add.s32 	%r32, %r1, 256;
	and.b32  	%r12, %r32, 511;
	add.s32 	%r33, %r1, 288;
	and.b32  	%r13, %r33, 511;
	add.s32 	%r34, %r1, 320;
	and.b32  	%r14, %r34, 511;
	add.s32 	%r35, %r1, 352;
	and.b32  	%r15, %r35, 511;
	add.s32 	%r36, %r1, 384;
	and.b32  	%r16, %r36, 511;
	add.s32 	%r37, %r1, 416;
	and.b32  	%r17, %r37, 511;
	add.s32 	%r38, %r1, 448;
	and.b32  	%r18, %r38, 511;
	add.s32 	%r39, %r1, 480;
	and.b32  	%r19, %r39, 511;
	mov.f32 	%f35, 0f00000000;
	mov.u32 	%r75, 0;

$L__BB1_1:
	mul.wide.u32 	%rd19, %r74, -1851608123;
	shr.u64 	%rd20, %rd19, 50;
	cvt.u32.u64 	%r40, %rd20;
	mul.lo.s32 	%r41, %r40, -460800;
	add.s32 	%r42, %r4, %r74;
	add.s32 	%r43, %r42, %r41;
	mul.wide.u32 	%rd21, %r43, 4;
	add.s64 	%rd3, %rd2, %rd21;
	// begin inline asm
	ld.global.cg.b32 %f4, [%rd3];
	// end inline asm
	add.s32 	%r44, %r5, %r74;
	add.s32 	%r45, %r44, %r41;
	mul.wide.u32 	%rd22, %r45, 4;
	add.s64 	%rd4, %rd2, %rd22;
	// begin inline asm
	ld.global.cg.b32 %f5, [%rd4];
	// end inline asm
	add.s32 	%r46, %r6, %r74;
	add.s32 	%r47, %r46, %r41;
	mul.wide.u32 	%rd23, %r47, 4;
	add.s64 	%rd5, %rd2, %rd23;
	// begin inline asm
	ld.global.cg.b32 %f6, [%rd5];
	// end inline asm
	add.s32 	%r48, %r7, %r74;
	add.s32 	%r49, %r48, %r41;
	mul.wide.u32 	%rd24, %r49, 4;
	add.s64 	%rd6, %rd2, %rd24;
	// begin inline asm
	ld.global.cg.b32 %f7, [%rd6];
	// end inline asm
	add.s32 	%r50, %r8, %r74;
	add.s32 	%r51, %r50, %r41;
	mul.wide.u32 	%rd25, %r51, 4;
	add.s64 	%rd7, %rd2, %rd25;
	// begin inline asm
	ld.global.cg.b32 %f8, [%rd7];
	// end inline asm
	add.s32 	%r52, %r9, %r74;
	add.s32 	%r53, %r52, %r41;
	mul.wide.u32 	%rd26, %r53, 4;
	add.s64 	%rd8, %rd2, %rd26;
	// begin inline asm
	ld.global.cg.b32 %f9, [%rd8];
	// end inline asm
	add.s32 	%r54, %r10, %r74;
	add.s32 	%r55, %r54, %r41;
	mul.wide.u32 	%rd27, %r55, 4;
	add.s64 	%rd9, %rd2, %rd27;
	// begin inline asm
	ld.global.cg.b32 %f10, [%rd9];
	// end inline asm
	add.s32 	%r56, %r11, %r74;
	add.s32 	%r57, %r56, %r41;
	mul.wide.u32 	%rd28, %r57, 4;
	add.s64 	%rd10, %rd2, %rd28;
	// begin inline asm
	ld.global.cg.b32 %f11, [%rd10];
	// end inline asm
	add.s32 	%r58, %r12, %r74;
	add.s32 	%r59, %r58, %r41;
	mul.wide.u32 	%rd29, %r59, 4;
	add.s64 	%rd11, %rd2, %rd29;
	// begin inline asm
	ld.global.cg.b32 %f12, [%rd11];
	// end inline asm
	add.s32 	%r60, %r13, %r74;
	add.s32 	%r61, %r60, %r41;
	mul.wide.u32 	%rd30, %r61, 4;
	add.s64 	%rd12, %rd2, %rd30;
	// begin inline asm
	ld.global.cg.b32 %f13, [%rd12];
	// end inline asm
	add.s32 	%r62, %r14, %r74;
	add.s32 	%r63, %r62, %r41;
	mul.wide.u32 	%rd31, %r63, 4;
	add.s64 	%rd13, %rd2, %rd31;
	// begin inline asm
	ld.global.cg.b32 %f14, [%rd13];
	// end inline asm
	add.s32 	%r64, %r15, %r74;
	add.s32 	%r65, %r64, %r41;
	mul.wide.u32 	%rd32, %r65, 4;
	add.s64 	%rd14, %rd2, %rd32;
	// begin inline asm
	ld.global.cg.b32 %f15, [%rd14];
	// end inline asm
	add.s32 	%r66, %r16, %r74;
	add.s32 	%r67, %r66, %r41;
	mul.wide.u32 	%rd33, %r67, 4;
	add.s64 	%rd15, %rd2, %rd33;
	// begin inline asm
	ld.global.cg.b32 %f16, [%rd15];
	// end inline asm
	add.s32 	%r68, %r17, %r74;
	add.s32 	%r69, %r68, %r41;
	mul.wide.u32 	%rd34, %r69, 4;
	add.s64 	%rd16, %rd2, %rd34;
	// begin inline asm
	ld.global.cg.b32 %f17, [%rd16];
	// end inline asm
	add.s32 	%r70, %r18, %r74;
	add.s32 	%r71, %r70, %r41;
	mul.wide.u32 	%rd35, %r71, 4;
	add.s64 	%rd17, %rd2, %rd35;
	// begin inline asm
	ld.global.cg.b32 %f18, [%rd17];
	// end inline asm
	add.s32 	%r72, %r19, %r74;
	add.s32 	%r73, %r72, %r41;
	mul.wide.u32 	%rd36, %r73, 4;
	add.s64 	%rd18, %rd2, %rd36;
	// begin inline asm
	ld.global.cg.b32 %f19, [%rd18];
	// end inline asm
	add.f32 	%f20, %f35, %f4;
	add.f32 	%f21, %f20, %f5;
	add.f32 	%f22, %f21, %f6;
	add.f32 	%f23, %f22, %f7;
	add.f32 	%f24, %f23, %f8;
	add.f32 	%f25, %f24, %f9;
	add.f32 	%f26, %f25, %f10;
	add.f32 	%f27, %f26, %f11;
	add.f32 	%f28, %f27, %f12;
	add.f32 	%f29, %f28, %f13;
	add.f32 	%f30, %f29, %f14;
	add.f32 	%f31, %f30, %f15;
	add.f32 	%f32, %f31, %f16;
	add.f32 	%f33, %f32, %f17;
	add.f32 	%f34, %f33, %f18;
	add.f32 	%f35, %f34, %f19;
	add.s32 	%r74, %r74, 512;
	add.s32 	%r75, %r75, 512;
	setp.lt.u32 	%p1, %r75, 460800;
	@%p1 bra 	$L__BB1_1;

	setp.ne.s32 	%p2, %r2, 0;
	@%p2 bra 	$L__BB1_4;

	cvta.to.global.u64 	%rd37, %rd1;
	mul.wide.u32 	%rd38, %r1, 4;
	add.s64 	%rd39, %rd37, %rd38;
	st.global.f32 	[%rd39], %f35;

$L__BB1_4:
	ret;

}
	// .globl	_Z6l2_bw3PfS_
.visible .entry _Z6l2_bw3PfS_(
	.param .u64 _Z6l2_bw3PfS__param_0,
	.param .u64 _Z6l2_bw3PfS__param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<17>;
	.reg .f32 	%f<69>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<74>;


	ld.param.u64 	%rd73, [_Z6l2_bw3PfS__param_1];
	mov.u32 	%r1, %tid.x;
	cvt.u16.u32 	%rs1, %r1;
	and.b32  	%r5, %r1, 511;
	mul.wide.u32 	%rd1, %r5, 4;
	add.s16 	%rs2, %rs1, 480;
	cvt.u32.u16 	%r6, %rs2;
	and.b32  	%r7, %r6, 511;
	mul.wide.u32 	%rd2, %r7, 4;
	add.s16 	%rs3, %rs1, 448;
	cvt.u32.u16 	%r8, %rs3;
	and.b32  	%r9, %r8, 511;
	mul.wide.u32 	%rd3, %r9, 4;
	add.s16 	%rs4, %rs1, 416;
	cvt.u32.u16 	%r10, %rs4;
	and.b32  	%r11, %r10, 511;
	mul.wide.u32 	%rd4, %r11, 4;
	add.s16 	%rs5, %rs1, 384;
	cvt.u32.u16 	%r12, %rs5;
	and.b32  	%r13, %r12, 511;
	mul.wide.u32 	%rd5, %r13, 4;
	add.s16 	%rs6, %rs1, 352;
	cvt.u32.u16 	%r14, %rs6;
	and.b32  	%r15, %r14, 511;
	mul.wide.u32 	%rd6, %r15, 4;
	add.s16 	%rs7, %rs1, 320;
	cvt.u32.u16 	%r16, %rs7;
	and.b32  	%r17, %r16, 511;
	mul.wide.u32 	%rd7, %r17, 4;
	add.s16 	%rs8, %rs1, 288;
	cvt.u32.u16 	%r18, %rs8;
	and.b32  	%r19, %r18, 511;
	mul.wide.u32 	%rd8, %r19, 4;
	add.s16 	%rs9, %rs1, 256;
	cvt.u32.u16 	%r20, %rs9;
	and.b32  	%r21, %r20, 511;
	mul.wide.u32 	%rd9, %r21, 4;
	add.s16 	%rs10, %rs1, 224;
	cvt.u32.u16 	%r22, %rs10;
	and.b32  	%r23, %r22, 511;
	mul.wide.u32 	%rd10, %r23, 4;
	add.s16 	%rs11, %rs1, 192;
	cvt.u32.u16 	%r24, %rs11;
	and.b32  	%r25, %r24, 511;
	mul.wide.u32 	%rd11, %r25, 4;
	add.s16 	%rs12, %rs1, 160;
	cvt.u32.u16 	%r26, %rs12;
	and.b32  	%r27, %r26, 511;
	mul.wide.u32 	%rd12, %r27, 4;
	add.s16 	%rs13, %rs1, 128;
	cvt.u32.u16 	%r28, %rs13;
	and.b32  	%r29, %r28, 511;
	mul.wide.u32 	%rd13, %r29, 4;
	add.s16 	%rs14, %rs1, 96;
	cvt.u32.u16 	%r30, %rs14;
	and.b32  	%r31, %r30, 511;
	mul.wide.u32 	%rd14, %r31, 4;
	add.s16 	%rs15, %rs1, 64;
	cvt.u32.u16 	%r32, %rs15;
	and.b32  	%r33, %r32, 511;
	mul.wide.u32 	%rd15, %r33, 4;
	add.s16 	%rs16, %rs1, 32;
	cvt.u32.u16 	%r34, %rs16;
	and.b32  	%r35, %r34, 511;
	mul.wide.u32 	%rd16, %r35, 4;
	or.b64  	%rd17, %rd1, 2048;
	or.b64  	%rd18, %rd2, 2048;
	or.b64  	%rd19, %rd3, 2048;
	or.b64  	%rd20, %rd4, 2048;
	or.b64  	%rd21, %rd5, 2048;
	or.b64  	%rd22, %rd6, 2048;
	or.b64  	%rd23, %rd7, 2048;
	or.b64  	%rd24, %rd8, 2048;
	or.b64  	%rd25, %rd9, 2048;
	or.b64  	%rd26, %rd10, 2048;
	or.b64  	%rd27, %rd11, 2048;
	or.b64  	%rd28, %rd12, 2048;
	or.b64  	%rd29, %rd13, 2048;
	or.b64  	%rd30, %rd14, 2048;
	or.b64  	%rd31, %rd15, 2048;
	or.b64  	%rd32, %rd16, 2048;
	mov.f32 	%f68, 0f00000000;
	mov.u32 	%r38, 0;

$L__BB2_1:
	add.s64 	%rd37, %rd73, %rd1;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd37];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd38, %rd73, %rd16;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd38];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd39, %rd73, %rd15;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd39];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd40, %rd73, %rd14;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd40];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd41, %rd73, %rd13;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd41];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd42, %rd73, %rd12;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd42];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd43, %rd73, %rd11;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd43];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd44, %rd73, %rd10;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd44];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd45, %rd73, %rd9;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd45];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd46, %rd73, %rd8;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd46];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd47, %rd73, %rd7;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd47];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd48, %rd73, %rd6;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd48];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd49, %rd73, %rd5;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd49];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd50, %rd73, %rd4;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd50];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd51, %rd73, %rd3;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd51];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd52, %rd73, %rd2;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd52];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd53, %rd73, %rd17;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd53];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd54, %rd73, %rd32;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd54];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd55, %rd73, %rd31;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd55];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd56, %rd73, %rd30;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd56];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd57, %rd73, %rd29;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd57];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd58, %rd73, %rd28;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd58];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd59, %rd73, %rd27;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd59];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd60, %rd73, %rd26;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd60];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd61, %rd73, %rd25;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd61];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd62, %rd73, %rd24;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd62];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd63, %rd73, %rd23;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd63];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd64, %rd73, %rd22;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd64];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd65, %rd73, %rd21;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd65];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd66, %rd73, %rd20;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd66];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd67, %rd73, %rd19;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd67];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd68, %rd73, %rd18;
	// begin inline asm
	{	
.reg .f32 data;
	ld.global.cg.f32 data, [%rd68];
	add.f32 %f68, data, %f68;
	}
	// end inline asm
	add.s64 	%rd73, %rd73, 4096;
	add.s32 	%r38, %r38, 1024;
	setp.lt.u32 	%p1, %r38, 460800;
	@%p1 bra 	$L__BB2_1;

	mov.u32 	%r36, %ctaid.x;
	setp.ne.s32 	%p2, %r36, 0;
	@%p2 bra 	$L__BB2_4;

	mov.u32 	%r37, %tid.x;
	ld.param.u64 	%rd72, [_Z6l2_bw3PfS__param_0];
	cvta.to.global.u64 	%rd69, %rd72;
	mul.wide.u32 	%rd70, %r37, 4;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f68;

$L__BB2_4:
	ret;

}

